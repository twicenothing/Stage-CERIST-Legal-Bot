import os
import json
import chromadb
from sentence_transformers import SentenceTransformer

# --- CONFIGURATION ---
JSON_DIR = "../../data/json_llm_extracted"  # L√† o√π safe_chunker a mis les fichiers
CHROMA_PATH = "../../data/chroma_db"        # Le dossier qu'on va recr√©er
COLLECTION_NAME = "legal_algeria"
MODEL_NAME = "BAAI/bge-m3"

def main():
    # 1. Initialiser ChromaDB
    print(f"üîÑ Initialisation de ChromaDB dans {CHROMA_PATH}...")
    client = chromadb.PersistentClient(path=CHROMA_PATH)
    
    # On supprime la collection si elle existe pour repartir de z√©ro (double s√©curit√©)
    try:
        client.delete_collection(COLLECTION_NAME)
        print("üóëÔ∏è Ancienne collection supprim√©e.")
    except:
        pass
    
    collection = client.create_collection(name=COLLECTION_NAME)

    # 2. Charger le mod√®le d'embedding
    print(f"ü§ñ Chargement du mod√®le {MODEL_NAME}...")
    model = SentenceTransformer(MODEL_NAME, device="cuda", model_kwargs={"use_safetensors": True})

    # 3. Lister les fichiers JSON
    if not os.path.exists(JSON_DIR):
        print(f"‚ùå Erreur : Le dossier {JSON_DIR} n'existe pas. Lance safe_chunker.py d'abord.")
        return

    files = [f for f in os.listdir(JSON_DIR) if f.endswith(".json")]
    print(f"üì¶ {len(files)} fichiers trouv√©s √† indexer.")

    # 4. Boucle d'indexation
    total_docs = 0
    
    for filename in files:
        file_path = os.path.join(JSON_DIR, filename)
        print(f"   üìÑ Traitement de {filename}...", end="")
        
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            # On v√©rifie que la structure est bonne (celle de safe_chunker)
            if "documents" not in data:
                print(" ‚ö†Ô∏è Pas de cl√© 'documents', fichier ignor√©.")
                continue

            documents = []
            ids = []
            metadatas = []
            embeddings = []

            # Extraction des donn√©es
            texts_to_embed = []
            
            for item in data["documents"]:
                doc_content = item.get("content", "")
                
                if not doc_content.strip():
                    continue

                # On pr√©pare les listes pour Chroma
                documents.append(doc_content)
                ids.append(item.get("id", f"{filename}_{total_docs}")) # Fallback ID
                metadatas.append({
                    "source": filename,
                    "title": item.get("title", filename)
                })
                texts_to_embed.append(doc_content)
                total_docs += 1

            # Calcul des embeddings (Vectorisation)
            if texts_to_embed:
                embeddings = model.encode(texts_to_embed).tolist()
                
                # Ajout dans la base
                collection.add(
                    documents=documents,
                    embeddings=embeddings,
                    metadatas=metadatas,
                    ids=ids
                )
                print(f" ‚úÖ {len(documents)} chunks index√©s.")
            else:
                print(" ‚ö†Ô∏è Aucun texte valide trouv√©.")

        except Exception as e:
            print(f" ‚ùå Erreur : {e}")

    print("\n" + "="*50)
    print(f"üéâ INDEXATION TERMIN√âE ! Total : {total_docs} fragments de texte.")
    print("="*50)

if __name__ == "__main__":
    main()